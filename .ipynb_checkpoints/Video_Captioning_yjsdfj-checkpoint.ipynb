{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sg1KOmZE9rPG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import pickle, functools, operator\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import joblib\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import json\n",
    "import random\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_KCDV5hs-WbU",
    "outputId": "f5c5cec5-f88f-4c55-9502-1cae035ffdbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17216\n",
      "3951\n",
      "14633\n",
      "2583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1450"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path=r'MLDS_hw2_data\\training_data'\n",
    "TRAIN_LABEL_PATH = os.path.join(train_path, 'training_label.json')\n",
    "# mentioning the train test split\n",
    "train_split = 0.85\n",
    "# loading the json file for training\n",
    "with open(TRAIN_LABEL_PATH) as data_file:    \n",
    "    y_data = json.load(data_file)\n",
    "# train_list contains all the captions with their video ID\n",
    "# vocab_list contains all the vocabulary from training data\n",
    "train_list = []\n",
    "vocab_list = []\n",
    "for y in y_data:\n",
    "  for caption in y['caption']:\n",
    "    caption = \"<bos> \" + caption + \" <eos>\"\n",
    "    # we are only using sentences whose length lie between 6 and 10\n",
    "    if len(caption.split())>10 or len(caption.split())<6:\n",
    "      continue\n",
    "    else:\n",
    "      train_list.append([caption, y['id']])\n",
    "print(len(train_list))\n",
    "random.shuffle(train_list)\n",
    "training_list = train_list[:int(len(train_list)*train_split)]\n",
    "validation_list = train_list[int(len(train_list)*train_split):]\n",
    "for train in training_list:\n",
    "    vocab_list.append(train[0])\n",
    "# Tokenizing the words\n",
    "tokenizer = Tokenizer(num_words=1500)\n",
    "tokenizer.fit_on_texts(vocab_list)\n",
    "print(len(tokenizer.word_index))\n",
    "x_data = {}\n",
    "TRAIN_FEATURE_DIR = os.path.join(r'MLDS_hw2_data\\training_data', 'feat')\n",
    "# Loading all the numpy arrays at once and saving them in a dictionary\n",
    "for filename in os.listdir(TRAIN_FEATURE_DIR):\n",
    "    f = np.load(os.path.join(TRAIN_FEATURE_DIR, filename))\n",
    "    x_data[filename[:-4]] = f\n",
    "print(len(training_list))\n",
    "print(len(validation_list))\n",
    "len(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "U82Pabyx-tBY"
   },
   "outputs": [],
   "source": [
    "# Creating a custom data generator because we cannot load so many files at once\n",
    "def load_datatest(train_path, epochs=100, x_data=x_data, tokenizer=tokenizer, num_decoder_tokens=1500,training_list=train_list, batch_size=320, maxlen=10):\n",
    "    encoder_input_data = []\n",
    "    decoder_input_data = []\n",
    "    decoder_target_data = []\n",
    "    videoId = []\n",
    "    videoSeq = []\n",
    "    # separating the videoId and the video captions\n",
    "    for idx, cap in enumerate(training_list):\n",
    "        caption = cap[0]\n",
    "        videoId.append(cap[1])\n",
    "        videoSeq.append(caption)\n",
    "    # converting the captions to tokens and padding them to equal sizes\n",
    "    train_sequences = tokenizer.texts_to_sequences(videoSeq)\n",
    "    train_sequences = np.array(train_sequences)\n",
    "    train_sequences = pad_sequences(train_sequences, padding='post',truncating='post', maxlen=maxlen)\n",
    "    max_seq_length = train_sequences.shape[1]\n",
    "    filesize = len(train_sequences)\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    vCount = 0\n",
    "    n = 0\n",
    "    for i in range(epochs):\n",
    "      for idx in  range(0,filesize):\n",
    "        n += 1\n",
    "        encoder_input_data.append(x_data[videoId[idx]])\n",
    "        y = to_categorical(train_sequences[idx], num_decoder_tokens)\n",
    "        decoder_input_data.append(y[:-1])\n",
    "        decoder_target_data.append(y[1:])\n",
    "        if n == batch_size:\n",
    "          encoder_input = np.array(encoder_input_data)\n",
    "          decoder_input = np.array(decoder_input_data)\n",
    "          decoder_target = np.array(decoder_target_data)\n",
    "          encoder_input_data = []\n",
    "          decoder_input_data = []\n",
    "          decoder_target_data = []\n",
    "          n = 0\n",
    "          yield ([encoder_input, decoder_input], decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8gOjWCig-wub"
   },
   "outputs": [],
   "source": [
    "# writing the train and validation generator\n",
    "train = load_datatest(train_path='training_data',batch_size=80, training_list=training_list, x_data=x_data, epochs=150)\n",
    "valid = load_datatest(train_path='training_data',batch_size=80, training_list=validation_list, x_data=x_data, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZACEJzuP-2ji"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "time_steps_encoder is the number of frames per video we will be using for training\n",
    "num_encoder_tokens is the number of features from each frame\n",
    "latent_dim is the number of hidden features for lstm\n",
    "time_steps_decoder is the maximum length of each sentence\n",
    "num_decoder_tokens is the final number of tokens in the softmax layer\n",
    "batch size\n",
    "\"\"\"\n",
    "time_steps_encoder=80\n",
    "num_encoder_tokens=4096\n",
    "latent_dim=512\n",
    "time_steps_decoder=10\n",
    "num_decoder_tokens=1500\n",
    "batch_size=80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "id": "YTORZjeFGLuI",
    "outputId": "8e293e0b-db00-471d-ec21-5ff603e94400"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, 80, 4096)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, 10, 1500)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "endcoder_lstm (LSTM)            [(None, 80, 512), (N 9439232     encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, 10, 512), (N 4122624     decoder_inputs[0][0]             \n",
      "                                                                 endcoder_lstm[0][1]              \n",
      "                                                                 endcoder_lstm[0][2]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_relu (Dense)            (None, 10, 1500)     769500      decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 14,331,356\n",
      "Trainable params: 14,331,356\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "# Setting up the encoder\n",
    "encoder_inputs = Input(shape=(time_steps_encoder, num_encoder_tokens), name=\"encoder_inputs\")\n",
    "encoder = LSTM(latent_dim, return_state=True,return_sequences=True, name='endcoder_lstm')\n",
    "_, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "# Set up the decoder\n",
    "decoder_inputs = Input(shape=(time_steps_decoder, num_decoder_tokens), name= \"decoder_inputs\")\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_relu')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()\n",
    "plot_model(model, to_file='model_train.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28unBh23G0mL",
    "outputId": "85aba68d-fdeb-4276-923d-79f06a5470e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\G Yuvan Shankar\\anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 182 steps, validate for 32 steps\n",
      "Epoch 1/150\n",
      "182/182 [==============================] - 73s 399ms/step - loss: 3.4793 - accuracy: 0.3988 - val_loss: 2.9854 - val_accuracy: 0.4467\n",
      "Epoch 2/150\n",
      "182/182 [==============================] - 79s 433ms/step - loss: 2.8256 - accuracy: 0.4694 - val_loss: 2.6494 - val_accuracy: 0.4936\n",
      "Epoch 3/150\n",
      "182/182 [==============================] - 75s 415ms/step - loss: 2.4794 - accuracy: 0.5331 - val_loss: 2.3530 - val_accuracy: 0.5559\n",
      "Epoch 4/150\n",
      "182/182 [==============================] - 73s 403ms/step - loss: 2.1895 - accuracy: 0.5780 - val_loss: 2.1287 - val_accuracy: 0.5856\n",
      "Epoch 5/150\n",
      "182/182 [==============================] - 73s 402ms/step - loss: 1.9591 - accuracy: 0.6099 - val_loss: 1.9463 - val_accuracy: 0.6132\n",
      "Epoch 6/150\n",
      "182/182 [==============================] - 74s 405ms/step - loss: 1.7733 - accuracy: 0.6362 - val_loss: 1.8135 - val_accuracy: 0.6303\n",
      "Epoch 7/150\n",
      "182/182 [==============================] - 69s 382ms/step - loss: 1.6247 - accuracy: 0.6569 - val_loss: 1.6966 - val_accuracy: 0.6490\n",
      "Epoch 8/150\n",
      "182/182 [==============================] - 67s 366ms/step - loss: 1.5023 - accuracy: 0.6747 - val_loss: 1.5985 - val_accuracy: 0.6618\n",
      "Epoch 9/150\n",
      "182/182 [==============================] - 77s 425ms/step - loss: 1.4024 - accuracy: 0.6891 - val_loss: 1.5380 - val_accuracy: 0.6709\n",
      "Epoch 10/150\n",
      "182/182 [==============================] - 73s 402ms/step - loss: 1.3193 - accuracy: 0.7021 - val_loss: 1.4855 - val_accuracy: 0.6763\n",
      "Epoch 11/150\n",
      "182/182 [==============================] - 66s 364ms/step - loss: 1.2506 - accuracy: 0.7117 - val_loss: 1.4342 - val_accuracy: 0.6829\n",
      "Epoch 12/150\n",
      "182/182 [==============================] - 68s 371ms/step - loss: 1.1900 - accuracy: 0.7199 - val_loss: 1.3912 - val_accuracy: 0.6905\n",
      "Epoch 13/150\n",
      "182/182 [==============================] - 72s 394ms/step - loss: 1.1397 - accuracy: 0.7275 - val_loss: 1.3591 - val_accuracy: 0.6958\n",
      "Epoch 14/150\n",
      "182/182 [==============================] - 73s 403ms/step - loss: 1.0951 - accuracy: 0.7338 - val_loss: 1.3373 - val_accuracy: 0.6978\n",
      "Epoch 15/150\n",
      "181/182 [============================>.] - ETA: 0s - loss: 1.0539 - accuracy: 0.7392WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 32 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "182/182 [==============================] - 62s 343ms/step - loss: 1.0534 - accuracy: 0.7393\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Empty training data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-c2c9fc1e6c2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     model.fit(train, validation_data=valid, validation_steps=(len(validation_list)//batch_size),\n\u001b[0;32m     17\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             callbacks=[x, earlystopping, tensorboard_callback])\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nW: interrupt received, stopping\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    393\u001b[0m                       \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                       \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m                       total_epochs=1)\n\u001b[0m\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[0;32m    397\u001b[0m                                  prefix='val_')\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m   \u001b[1;31m# End of an epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m   \u001b[0maggregator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0maggregator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mfinalize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    142\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Empty training data.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Empty training data."
     ]
    }
   ],
   "source": [
    "# Early Stopping\n",
    "epochs=150\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience = 5, verbose=1, mode='min')\n",
    "\n",
    "# Tensorboard callback\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "# Run training\n",
    "opt = tf.keras.optimizers.Adam(lr = 0.0003)\n",
    "x = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1,patience=2,verbose=0,mode=\"auto\")\n",
    "model.compile(metrics=['accuracy'], optimizer=opt, loss='categorical_crossentropy')\n",
    "\n",
    "\n",
    "try:\n",
    "    model.fit(train, validation_data=valid, validation_steps=(len(validation_list)//batch_size),\n",
    "        epochs=epochs, steps_per_epoch=(len(training_list)//batch_size),\n",
    "            callbacks=[x, earlystopping, tensorboard_callback])\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nW: interrupt received, stopping\")\n",
    "finally:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 758
    },
    "id": "6NVfxmnTZkzF",
    "outputId": "faa44979-8ff2-4eb9-a2b5-3cf7240148f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_inputs (InputLayer)  [(None, 80, 4096)]        0         \n",
      "_________________________________________________________________\n",
      "endcoder_lstm (LSTM)         [(None, 80, 512), (None,  9439232   \n",
      "=================================================================\n",
      "Total params: 9,439,232\n",
      "Trainable params: 9,439,232\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_inputs (InputLayer)     [(None, 10, 1500)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, 10, 512), (N 4122624     decoder_inputs[0][0]             \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_relu (Dense)            (None, 10, 1500)     769500      decoder_lstm[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,892,124\n",
      "Trainable params: 4,892,124\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "save_model_path = 'model_final'\n",
    "if not os.path.exists(save_model_path):\n",
    "    os.makedirs(save_model_path)\n",
    "\n",
    "# Saving encoder as in training\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Saving decoder states and dense layer \n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "encoder_model.summary()\n",
    "decoder_model.summary()\n",
    "encoder_model.save(os.path.join(save_model_path, 'encoder_model.h5'))\n",
    "decoder_model.save_weights(os.path.join(save_model_path, 'decoder_model_weights.h5'))\n",
    "with open(os.path.join(save_model_path,'tokenizer'+ str(num_decoder_tokens) ),'wb') as file:\n",
    "    joblib.dump(tokenizer, file)\n",
    "plot_model(encoder_model, to_file='model_inference_encoder.png', show_shapes=True, show_layer_names=True)\n",
    "plot_model(decoder_model, to_file='model_inference_decoder.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "AH1LU-ftZpJc",
    "outputId": "eb34e952-3cef-4e14-8b59-5e6bf2bea117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(encoder_model, to_file='model_inference_encoder.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "gn7acHsca7n3"
   },
   "outputs": [],
   "source": [
    "# class to perform inference on all test files and save as test_output.txt\n",
    "class Video2Text(object):\n",
    "    ''' Initialize the parameters for the model '''\n",
    "    def __init__(self):\n",
    "        self.latent_dim = 512\n",
    "        self.num_encoder_tokens = 4096\n",
    "        self.num_decoder_tokens = 1500\n",
    "        self.time_steps_encoder = 80\n",
    "        self.time_steps_decoder = None\n",
    "        self.preload = True\n",
    "        self.preload_data_path = 'preload_data'\n",
    "        self.max_probability = -1\n",
    "\n",
    "        # processed data\n",
    "        self.encoder_input_data = []\n",
    "        self.decoder_input_data = []\n",
    "        self.decoder_target_data = []\n",
    "        self.tokenizer = None\n",
    "\n",
    "        # models\n",
    "        self.encoder_model = None\n",
    "        self.decoder_model = None\n",
    "        self.inf_encoder_model = None\n",
    "        self.inf_decoder_model = None\n",
    "        self.save_model_path = 'model_final'\n",
    "        self.test_path = 'testing_data'\n",
    "    def load_inference_models(self):\n",
    "        # load tokenizer\n",
    "        \n",
    "        with open(os.path.join(self.save_model_path, 'tokenizer' + str(self.num_decoder_tokens)), 'rb') as file:\n",
    "            self.tokenizer = joblib.load(file)\n",
    "\n",
    "        # inference encoder model\n",
    "        self.inf_encoder_model = load_model(os.path.join(self.save_model_path, 'encoder_model.h5'))\n",
    "\n",
    "        # inference decoder model\n",
    "        decoder_inputs = Input(shape=(None, self.num_decoder_tokens))\n",
    "        decoder_dense = Dense(self.num_decoder_tokens, activation='softmax')\n",
    "        decoder_lstm = LSTM(self.latent_dim, return_sequences=True, return_state=True)\n",
    "        decoder_state_input_h = Input(shape=(self.latent_dim,))\n",
    "        decoder_state_input_c = Input(shape=(self.latent_dim,))\n",
    "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "        decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "        decoder_states = [state_h, state_c]\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        self.inf_decoder_model = Model(\n",
    "            [decoder_inputs] + decoder_states_inputs,\n",
    "            [decoder_outputs] + decoder_states)\n",
    "        self.inf_decoder_model.load_weights(os.path.join(self.save_model_path, 'decoder_model_weights.h5'))\n",
    "    \n",
    "    def decode_sequence2bs(self, input_seq):\n",
    "        states_value = self.inf_encoder_model.predict(input_seq)\n",
    "        target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
    "        target_seq[0, 0, self.tokenizer.word_index['bos']] = 1\n",
    "        self.beam_search(target_seq, states_value,[],[],0)\n",
    "        return decode_seq\n",
    "\n",
    "    def beam_search(self, target_seq, states_value, prob,  path, lens):\n",
    "        global decode_seq\n",
    "        node = 2\n",
    "        output_tokens, h, c = self.inf_decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "        output_tokens = output_tokens.reshape((self.num_decoder_tokens))\n",
    "        sampled_token_index = output_tokens.argsort()[-node:][::-1]\n",
    "        states_value = [h, c]\n",
    "        for i in range(node):\n",
    "            if sampled_token_index[i] == 0:\n",
    "                sampled_char = ''\n",
    "            else:\n",
    "                sampled_char = list(self.tokenizer.word_index.keys())[list(self.tokenizer.word_index.values()).index(sampled_token_index[i])]\n",
    "            MAX_LEN = 15\n",
    "            if(sampled_char != 'eos' and lens <= MAX_LEN):\n",
    "                p = output_tokens[sampled_token_index[i]]\n",
    "                if(sampled_char == ''):\n",
    "                    p = 1\n",
    "                prob_new = list(prob)\n",
    "                prob_new.append(p)\n",
    "                path_new = list(path)\n",
    "                path_new.append(sampled_char)\n",
    "                target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
    "                target_seq[0, 0, sampled_token_index[i]] = 1.\n",
    "                self.beam_search(target_seq, states_value, prob_new, path_new, lens+1)\n",
    "            else:\n",
    "                p = output_tokens[sampled_token_index[i]]\n",
    "                prob_new = list(prob)\n",
    "                prob_new.append(p)\n",
    "                p = functools.reduce(operator.mul, prob_new, 1)\n",
    "                if(p > self.max_probability):\n",
    "                    decode_seq = path\n",
    "                    self.max_probability = p\n",
    "\n",
    "    def decoded_sentence_tuning(self, decoded_sentence):\n",
    "        decode_str = []\n",
    "        filter_string = ['bos', 'eos']\n",
    "        unigram = {}\n",
    "        last_string = \"\"\n",
    "        for idx2, c in enumerate(decoded_sentence):\n",
    "            if c in unigram:\n",
    "                unigram[c] += 1\n",
    "            else:\n",
    "                unigram[c] = 1\n",
    "            if(last_string == c and idx2 > 0):\n",
    "                continue\n",
    "            if c in filter_string:\n",
    "                continue\n",
    "            if len(c) > 0:\n",
    "                decode_str.append(c)\n",
    "            if idx2 > 0:\n",
    "                last_string = c\n",
    "        return decode_str\n",
    "\n",
    "    def get_test_data(self, path):\n",
    "        X_test = []\n",
    "        X_test_filename = []\n",
    "        with open (os.path.join(path, 'testing_id.txt')) as testing_file:\n",
    "            lines = testing_file.readlines()\n",
    "            for filename in lines:\n",
    "                filename = filename.strip()\n",
    "                f = np.load(os.path.join(path , 'feat', filename + '.npy'))\n",
    "                X_test.append(f)\n",
    "                X_test_filename.append(filename[:-4])\n",
    "            X_test = np.array(X_test)\n",
    "        return X_test, X_test_filename\n",
    "\n",
    "    def test(self):\n",
    "        X_test, X_test_filename = self.get_test_data(os.path.join(self.test_path))\n",
    "        print(len(X_test), len(X_test_filename))\n",
    "        # generate inference test outputs\n",
    "        with open(os.path.join(self.save_model_path, 'test_output.txt'), 'w') as file:\n",
    "            for idx, x in enumerate(X_test): \n",
    "                file.write(X_test_filename[idx]+',')\n",
    "                decoded_sentence = self.decode_sequence2bs(x.reshape(-1, 80, 4096))\n",
    "                decode_str = self.decoded_sentence_tuning(decoded_sentence)\n",
    "                for d in decode_str:\n",
    "                    file.write(d + ' ')\n",
    "                file.write('\\n')\n",
    "                # re-init max prob\n",
    "                self.max_probability = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Es2Lu-Qa-kw",
    "outputId": "05191e00-d49a-4efc-9414-9ba08e9b769f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "c = Video2Text()\n",
    "c.load_inference_models()\n",
    "c.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Video Captioning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
